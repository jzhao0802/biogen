---
title: "Bayesian inference"
author: "Norman Poh"
date: "20 October 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Local set up

1. First, mount your data folder in Windows command prompt:
subst L: C:\Users\npoh\Documents\myProjects\Biogen_Tecfidera

2. set up local data drive
subst D: C:\Users\npoh\Documents\myProjects\Biogen_Tecfidera

```{r}
rm(list = ls())
setwd("L:/modelling")
library(palab)
library(palabmod)
library(ggplot2)
library(tidyverse)
library(stringr)
library(lubridate)
library(tictoc)
library(hashmap)
library(xgboost)
library(R.utils)
library(ROCR)
library(pROC) # to calculate auc
library(glmnet)
library(dismo)
source("L:/Lib/analyse_res_cv.R")
source("L:/Lib/calculate_eer.R")
source("L:/Lib/config_with_colnames.R")
source("L:/Lib/create_date_diffs.R")
source("L:/Lib/cut_linux.R")
source("L:/Lib/factor2numeric.R")
source("L:/Lib/make_negative.R")
source("L:/Lib/make_positive.R")
source("L:/Lib/make_var_name.R")
source("L:/Lib/mystat.R")
source("L:/Lib/plot_cond_density.R")
source("L:/Lib/remove_inf.R")
source("L:/Lib/remove_na.R")
source("L:/Lib/write_xgb_model.R")

```

```{r}

results_dir = "D:/Results/modelling_09_bayesian/"
mkdirs(results_dir)
```

## Load the original variables

```{r}
combined = readRDS("D:/Data/Processed/combined_date_complied_rectified_num_gaba_copay_data.rds")
config = read_csv("D:/Data/Processed/combined_date_complied_rectified_num_gaba_copay_config.csv")

```

## Get the map up

```{r}
Description = hashmap(config$Column, config$Description)
var_grouping = hashmap(config$Column, config$var_grouping)
var_period = hashmap(config$Column, config$var_period)

```

## Bayesian inference helper functions
```{r}

fit_kde <- function(x) {x
  kde = density(x, na.rm = TRUE)
  f=approxfun(kde$x, kde$y, yleft=0, yright=0)
  return(f)
}

get_feature_space <- function(feature) {
  stat_ = mystat(feature)
  x = seq(stat_[1], stat_[2],length=100)
  return(x)
}

train_bayes <- function(feature, label, model_prior) {
  
  N_0 = sum(label==0)
  N_1 = sum(label==1)
  x_space = get_feature_space(feature)
  
  if (N_1 <=2) { # use normal distribution when n<=2
    #llh_ = dnorm(x_space, mean(feature [label==1]), sd = sd(feature [label==1]))
    #llh_1 =approxfun(x_space, llh_, yleft=0, yright=0)
    llh_1 = model_prior$llh1
  } else {
    llh_1 = fit_kde(feature [label==1])
  }
  
  if (N_0 <=2) { # use normal distribution when n<=2
    #llh_ = dnorm(x_space, mean(feature [label==0]), sd = sd(feature [label==0]))
    #llh_0 =approxfun(x_space, llh_, yleft=0, yright=0)
    llh_0 = model_prior$llh0
  } else {
    llh_0 = fit_kde(feature [label==0] )
  }
  
  
  return(list(llh0 = llh_0, llh1 = llh_1, 
              N0 = N_0, N1 = N_1,
              x_space = x_space))
}

predict_bayes <- function(model, feature) {
  scores = model$llh1(feature) /(model$llh1(feature) + model$llh0(feature))
  return(scores)
}

predict_bayes_logit <- function(model, feature) {
  scores = log(model$llh1(feature)+.Machine$double.xmin) -
    log(model$llh0(feature)+.Machine$double.xmin)
  selected = is.na(scores)
  scores[selected] = 0
  return(scores)
}

inference_bayes <- function(model_bayes, x, legend_label = 'Prob-Bayes') {
  
  if (missing(x)) {
    x = model_bayes$x_space
  }
  
  prob_ = predict_bayes(model_bayes, x)
  mat =  rbind( tibble(x=x, y = model_bayes$llh0(x) * 20, type='llh 0'),
              tibble(x=x, y = model_bayes$llh1(x) * 20, type='llh 1'),
              tibble(x=x, y = prob_ , type =legend_label)
              )
}

remove_na_features <- function(fea_) {
  selected = !is.na(fea_)
  table_ = tibble(
  x = fea_[selected], label = combined$discontinue_flg[selected])
  return(table_)
}

adapt_bayes <- function(model_prior, model_indi, relevance_factor=100, x) {
  if (missing(x)) {
    x = model_prior$x_space
  }
  
  alpha0 = model_indi$N0 / (model_indi$N0 + relevance_factor)
  alpha1 = model_indi$N1 / (model_indi$N1 + relevance_factor)
  
  print( sprintf('relevance factor: %d', relevance_factor) )
  print( sprintf('alpha0: %1.3f, N0: %d', alpha0, model_indi$N0 ) )
  print( sprintf('alpha1: %1.3f, N1: %d', alpha1, model_indi$N1 ) )
  
  #compute the new likelihood function
  llh0_hat = model_indi$llh0(x) * alpha0 + model_prior$llh0(x) * (1-alpha0)
  llh0_fun =approxfun(x, llh0_hat, yleft=0, yright=0)
  
  llh1_hat = model_indi$llh1(x) * alpha1 + model_prior$llh1(x) * (1-alpha1)
  llh1_fun =approxfun(x, llh1_hat, yleft=0, yright=0)
  return(list(llh0 = llh0_fun, llh1=llh1_fun, 
    N0 = model_indi$N0, N1 = model_indi$N1))
}

```

## test it on a particular feature set

```{r}

model = train_bayes(combined$post_ae_fst1_diff, combined$discontinue_flg)
x = get_feature_space(combined$post_ae_fst1_diff)
prob_ = predict_bayes(model, x)

mat__ = inference_bayes(model, x)


# cdf = integrate(f, -Inf, 200)  # replace '2' by any other value.
#melt(data, id.vars, measure.vars, variable.name = "variable"
  
# train glm
#combined$discontinue_flg = as.factor(combined$discontinue_flg)

dat_ = combined %>% select( one_of(c('post_ae_fst1_diff','discontinue_flg')))
dat_ = dat_[!is.na(dat_$post_ae_fst1_diff),]
                     
target_tab = as.numeric(table(dat_$discontinue_flg))
iw = 1/target_tab[as.numeric(dat_$discontinue_flg)+1] 

#checking
sum(iw[dat_$discontinue_flg==1])
sum(iw[dat_$discontinue_flg==0])

#training
glm_ = glmnet(x=cbind(dat_$post_ae_fst1_diff, 1),
                      y=as.factor(dat_$discontinue_flg), 
                      family="binomial",
              weights = iw)

prob_lr = predict(glm_, cbind(x, 1), type='response', s=0.01)

mat =  rbind( mat__,
  tibble(x=x, y = as.numeric(prob_lr) , type ='Prob-LR'))

```
## Plotting

```{r}
ggplot(data=mat, aes(x=x, y=y, group = type, colour = type)) +
    geom_line() +
    geom_point( size=2, shape=21)  + #, fill="white")
    xlab(Description[['post_ae_fst1_diff']]) +
    ylab('Likelihood x 20, Probability')

ggplot(data=mat %>% filter( type %in% c('llh 0','llh 1')), 
       aes(x=x, y=y, group = type, colour = type)) +
    geom_line() +
    geom_point( size=2, shape=21)  + #, fill="white")
    xlab(Description[['post_ae_fst1_diff']]) +
    ylab('Likelihood')

```

## Define the variables not used in by model 3

```{r}
vlist = vector('list', 3)
vlist[[2]] = c('post_symps_fst8_diff','post_symps_fst10_diff','post_symps_fst12_diff','post_symps_fst13_diff','post_symps_fst14_diff','post_symps_fst15_diff','post_symps_fst16_diff')

vlist[[3]] = c('post_dme_fst_diff')

vlist[[1]] = c('pre_ae_num2','pre_comdx_num2','pre_comdx_num4','pre_comdx_num6','pre_comdx_num8','pre_comdx_num9','pre_comdx_num10','pre_comdx_num12','pre_comdx_num14','pre_comdx_num15','pre_comdx_num16','pre_comdx_num18','pre_comdx_num19','pre_comdx_num21','pre_comdx_num22','pre_comdx_num23','pre_comdx_num24','pre_comdx_num25','pre_comdx_num26','pre_concom_cdes_num1','pre_concom_gpif_num8','pre_concom_gpif_num12','pre_concom_gpif_num16','pre_concom_gpif_num18','pre_concom_gpif_num19','pre_concom_gpif_num21','pre_concom_gpif_num22','pre_concom_ndc_num2','pre_concom_ndc_num4','pre_concom_ndc_num6','pre_medication_num11','pre_medication_num13','pre_medication_num14','pre_medication_num15','pre_sympf_num1','pre_sympf_num4','pre_sympf_num5','pre_symps_num2','pre_symps_num4','pre_symps_num5','pre_symps_num6','pre_symps_num7','pre_symps_num8','pre_symps_num10','pre_symps_num12','pre_symps_num13','pre_symps_num14','pre_symps_num15','pre_symps_num16','pre_sub_confinements','pre_comdx_lst4_diff','pre_comdx_lst8_diff','pre_comdx_lst14_diff','pre_comdx_lst15_diff','pre_comdx_lst16_diff','pre_comdx_lst21_diff','pre_comdx_lst22_diff','pre_comdx_lst23_diff','pre_comdx_lst24_diff','pre_comdx_lst26_diff','pre_concom_gpif_lst8_diff','pre_concom_gpif_lst12_diff','pre_concom_gpif_lst18_diff','pre_concom_gpif_lst19_diff','pre_concom_gpif_lst21_diff','pre_concom_ndc_lst4_diff','pre_concom_ndc_lst6_diff','pre_medication_lst11_diff','pre_medication_lst13_diff','pre_medication_lst14_diff','pre_medication_lst15_diff','pre_symps_lst4_diff','pre_symps_lst5_diff','pre_symps_lst6_diff','pre_symps_lst7_diff','pre_symps_lst8_diff','pre_symps_lst10_diff','pre_symps_lst13_diff','pre_symps_lst14_diff','pre_symps_lst15_diff','pre_symps_lst16_diff','pre_script_lst_dayssup_diff','pre_concom_gpif_num2','pre_medication_num27')
```

# Get the prior models
```{r}
model_prior = vector('list', 3)

for (m in 2:3) {

  #collect all the variables observable in varlist for model m
  
  #get the variable -- we focus on the diff features only
  varlist_m = config$Column[ config[, paste0('v', m)] == TRUE ]
  varlist_m = varlist_m [ str_detect(varlist_m, '_diff') ]
  
  # the variable for which we have a lot of data
  varlist_m = setdiff(varlist_m, vlist[[m]])
  
  #collect all the scores together
  table_ = vector('list', length(varlist_m))
  for (j in 1:length(varlist_m)) {
      fea_ = combined[, varlist_m[j]] 
      selected = !is.na(fea_)
      table_[[j]] = tibble(
        x = fea_[selected], label = combined$discontinue_flg[selected], 
        var = varlist_m[j])
  }
  
  table__ = bind_rows(table_)
  
  model_prior[[m]] = train_bayes(table__$x, table__$label)
  x_ = get_feature_space(table__$x)  
  mat__ = inference_bayes(model_prior[[m]], x_)

  gg = ggplot(data=mat__, aes(x=x, y=y, group = type, colour = type)) +
    geom_line() +
    geom_point( size=2, shape=21)  + #, fill="white")
    xlab(Description[['general']]) +
    ylab('Likelihood x 20, Probability')
  print(gg)  
  
}
saveRDS(model_prior,'D:/Results/modelling_09_bayesian/model/model_prior.rds')
saveRDS(vlist,'D:/Results/modelling_09_bayesian/model/vlist.rds')
```

##  Fit the individual left out variables 
```{r}
model_indi = vector('list', 3)

for (m in 2:3) {
  model_indi[[m]] = vector('list', length(vlist[[m]]))
  
  x = model_prior[[m]]$x_space
  
  mat_prior = inference_bayes(model_prior[[m]], x, 
      legend_label='Prob.apriori')

  for (i in 1:length(vlist[[m]])) {
    
    #get the individual distribution from the few samples that we have
    mat__ = remove_na_features(combined[,vlist[[m]][i]])  
    
    # training -- here, we also include the prior model in case that there is less than two data points for training
    model_indi_ = train_bayes(mat__$x, mat__$label, model_prior[[m]])
  
    # adaptation
    model_indi[[m]][[i]] = adapt_bayes(model_prior[[m]], 
      model_indi_, relevance_factor=100,x )
    
    # make inference
    mat_indi_orig = inference_bayes(model_indi_, x, 
      legend_label='Prob.Original')
    mat_indi_adapted = inference_bayes(model_indi[[m]][[i]], x,
      legend_label='Prob.Adapted')
    
    # visualise
    mat__ = rbind( mat_indi_orig, 
                   mat_prior %>% filter( type=='Prob.apriori' ),
                   mat_indi_adapted %>% filter( type=='Prob.Adapted' ))

    title_ = sprintf('%s: #neg = %d, #pos = %d', 
      vlist[[m]][i], model_indi_$N0, model_indi_$N1)
    gg = ggplot(data=mat__, aes(x=x, y=y, group = type, colour = type)) +
    geom_line() +
    geom_point( size=2, shape=21)  + #, fill="white")
    xlab(Description[['general']]) +
    ylab('Probability, Likelihood x20') + ggtitle(title_)
    
    print(gg)  
    
    fname = sprintf( "D:/Results/modelling_09_bayesian/01_bayesian_inference__density_prob__%s.png",vlist[[m]][i] )

  ggsave(fname, plot = last_plot())
    
  }  
}

```
## kfold

```{r}



```